{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s226TJEMhDUB"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_openai langsmith langchain_community gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP-qLTaa9wfm"
      },
      "source": [
        "\n",
        "\n",
        "**Open .env file in this folder and observe that we have configured OPENAI_API_KEY. Replace it with your own key or key given by me**\n",
        "\n",
        "The Code in the below cell will load the .env file and set environment variables.\n",
        "\n",
        "**Write the code in the below cell and execute it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6wvVRSc9wfo"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFqXNNWejL0y"
      },
      "source": [
        "**Let us see if the model remembers previous messages**\n",
        "\n",
        "If you execute the below code, you will understand that the model will not remember our previous messages by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJVl7qO2iezM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import  ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "model = ChatOpenAI()\n",
        "firstresponse = model.invoke([HumanMessage(content=\"Hi! I'm Siva\")])\n",
        "\n",
        "print(firstresponse.content)\n",
        "secondresponse = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
        "\n",
        "print(secondresponse.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScCkjH02i_5z"
      },
      "source": [
        "**Now Let us Pass all the messages manually to the model and observe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfuUD9s7iqOs"
      },
      "outputs": [],
      "source": [
        "thirdresponse = model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Siva\"),\n",
        "        AIMessage(content=\"Hello Siva! How can I assist you today?\"),\n",
        "        HumanMessage(content=\"What's my name?\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(thirdresponse.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGsMBXsljW3b"
      },
      "source": [
        "**Now let us add chatmessagehistory manually**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jMgtfEdjd9b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "chain = prompt | model\n",
        "chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhDR-MLSoGvd"
      },
      "source": [
        "**Now, Let us create ChatMessageHistory , add messages and responses manually**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dRvDbXPj4g7"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "\n",
        "chat_message_history = ChatMessageHistory()\n",
        "\n",
        "chat_message_history.add_user_message(\n",
        "    \"Translate this sentence from English to French: I love programming.\"\n",
        ")\n",
        "response = chain.invoke(\n",
        "    {\"messages\": chat_message_history.messages}\n",
        ")\n",
        "response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTBE8AdE9wfr"
      },
      "source": [
        "**Now, let us add the response from AI into chat message history**\n",
        "\n",
        "**Then add our new usermessage also into chat message history**\n",
        "\n",
        "**If you invoke the chain with all messages in chat message history, you will get response which makes us feel like LLM has memory**\n",
        "\n",
        "**Understand and execute below code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h5rdaILoBFV"
      },
      "outputs": [],
      "source": [
        "chat_message_history.add_ai_message(AIMessage(response.content))\n",
        "#chat_message_history.messages\n",
        "\n",
        "chat_message_history.add_user_message(\"What did i ask u?\")\n",
        "\n",
        "finalresponse =  chain.invoke({ \"messages\": chat_message_history.messages })\n",
        "\n",
        "chat_message_history.add_ai_message(AIMessage(finalresponse.content))\n",
        "(chat_message_history.messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAsriXKosPM"
      },
      "source": [
        "**Now let us use RunnableWithMessageHistory. Let us see how it maintains message history automatically**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAfuZu-TppfO"
      },
      "source": [
        "**Firstly, let us define a function which returns an instance of ChatMessageHistory based on session_id which is default configurable parameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1KRamhno6lM"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igY9UxjTtFwl"
      },
      "source": [
        "**Observe how we are passing get_session_history to RunnableWithMessageHistory Internally, RunnableWithMessageHistory will use get_session_history to get BaseChatMessageHistory for current session**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ipZnzjV9wfs"
      },
      "source": [
        "Understand the below code and execute it in a cell.\n",
        "The below code goes through loop for 2 times asking you to enter your message.\n",
        "\n",
        "For the first time, enter the message \"My name is Siva\"\n",
        "Second time enter the message \"What is my name?\"\n",
        "\n",
        "Once u get response, you will understand that how memory works\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpkclqn6qLuM"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "count =0\n",
        "config = {\"configurable\": {\"session_id\": \"1122\"}}\n",
        "\n",
        "model_with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
        "\n",
        "while(count < 2):\n",
        "    content = input(\"Enter your message >> \")\n",
        "    result = model_with_message_history.invoke(\n",
        "        [HumanMessage(content=content)],\n",
        "        config=config,\n",
        "    )\n",
        "    count = count + 1\n",
        "    print(result.content)\n",
        "\n",
        "\n",
        "model_with_message_history.get_session_history(\"1122\").messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46c0oYnq9wfs"
      },
      "source": [
        "# Let us build a chat bot which has memory\n",
        "\n",
        "**Understand the below code and execute it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NLPckAQKpeR"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "#session_id = input(\"Enter a session id >> \")\n",
        "\n",
        "def predict(message, history):\n",
        "   config = {\"configurable\": {\"session_id\": \"1122\"}}\n",
        "   print(config)\n",
        "   model_with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
        "\n",
        "   response1= model_with_message_history.invoke(\n",
        "        [HumanMessage(content=message)],\n",
        "        config=config,\n",
        "    ).content\n",
        "   print(response1)\n",
        "\n",
        "   print(model_with_message_history.get_session_history(\"1122\").messages)\n",
        "   return response1\n",
        "\n",
        "\n",
        "\n",
        "gr.ChatInterface(predict).launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdDNGd4ytnMO"
      },
      "source": [
        "**Now Let us use ChatPromptTemplate with RunnableWithMessageHistory**\n",
        "\n",
        "Define a prompt using ChatPromptTemplate and then create a chain using it  as shown below\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaM4BqiQtxyO"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [  (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability .\",\n",
        "        ),  MessagesPlaceholder(variable_name=\"mychat_history\"),\n",
        "        (\"human\" ,\"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain =  prompt | model\n",
        "chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NchqHeey9wfs"
      },
      "source": [
        "**Observe how we have Created  RunnableWithMessageHistory using the above chain. Also Observe input_messages_key and  history_messages_key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em9iBYNQuAJW"
      },
      "outputs": [],
      "source": [
        "model_with_message_history = RunnableWithMessageHistory(\n",
        "    chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"mychat_history\"\n",
        ")\n",
        "model_with_message_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sevpsfQ9wft"
      },
      "source": [
        "**Understand how we are creating a chatbot using the below code and execute it in a cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEOfBqJ9hWkD"
      },
      "outputs": [],
      "source": [
        "def predict(message, history):\n",
        "   config = {\"configurable\": {\"session_id\": \"1298\"}}\n",
        "   print(config)\n",
        "\n",
        "\n",
        "   response1= model_with_message_history.invoke(\n",
        "        {\"input\": message},\n",
        "        config=config,\n",
        "    ).content\n",
        "   print(response1)\n",
        "\n",
        "   print(model_with_message_history.get_session_history(\"1298\").messages)\n",
        "   return response1\n",
        "\n",
        "\n",
        "\n",
        "gr.ChatInterface(predict).launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYq-E1nTwuzj"
      },
      "source": [
        "**Let us see how to stream the result**\n",
        "\n",
        "**Execute the below code in   cell and understand it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGGdkIqBwFT0"
      },
      "outputs": [],
      "source": [
        "content = input(\">> \")\n",
        "\n",
        "streamtresult = model_with_message_history.stream(\n",
        "       {\"input\": content},\n",
        "        config=config,\n",
        "    )\n",
        "for token in streamtresult:\n",
        "  print(token.content,end='|')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQxxi7AHxAez"
      },
      "source": [
        "**Let is understand how to customize**\n",
        "\n",
        "get_session_history functioned defined below uses a combination of user_id and conversation_id\n",
        "\n",
        "Observe how we are customizing  RunnableWithMessageHistory using ConfigurableFieldSpec\n",
        "\n",
        "**Execute the below cell and understand it**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD7nfzuqxE1U"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(user_id: str, conversation_id: str) -> ChatMessageHistory:\n",
        "    if (user_id, conversation_id) not in store:\n",
        "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
        "    return store[(user_id, conversation_id)]\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "model_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\", history_messages_key=\"mychat_history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"user_id\", annotation=str, \n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"conversation_id\", annotation=str, \n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"user_id\": \"1\", \"conversation_id\": \"1\"}}\n",
        "count =0\n",
        "while(count < 2):\n",
        "    content = input(\">> \")\n",
        "    result = model_with_message_history.invoke(\n",
        "       {\"input\": content,\"language\" : \"Hindi\"},\n",
        "        config=config,\n",
        "    )\n",
        "    count = count + 1\n",
        "\n",
        "    print(result.content)\n",
        "model_with_message_history.get_session_history(\"123\",\"1\").messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHOd-b0yg80"
      },
      "source": [
        "**Using FileChatMessageHistory to persist messages**\n",
        "\n",
        "**Execute the below code to understand how chatmessage history is saved to a file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzbPH5Azyllb"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import FileChatMessageHistory\n",
        "\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
        "    if (user_id, conversation_id) not in store:\n",
        "        store[(user_id, conversation_id)] = FileChatMessageHistory(user_id++conversation_id++\"savedconversationmessages.json\")\n",
        "    return store[(user_id, conversation_id)]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
