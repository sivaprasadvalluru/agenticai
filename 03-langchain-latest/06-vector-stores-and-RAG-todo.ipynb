{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Lab: Vector Stores and Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this practice lab, you will work with vector stores and implement a Retrieval Augmented Generation (RAG) system. Complete the TODOs and follow the hints to build your understanding of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the necessary packages. Execute the cell below to install all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install the required packages for working with LangChain, vector stores, and RAG\n",
    "# HINT: You need packages for LangChain, HuggingFace, OpenAI, Chroma, FAISS, PDF processing, and Gradio\n",
    "\n",
    "# pip install langchain-huggingface langchain_core langchain_openai langchain_chroma langchain_community faiss-cpu pypdf gradio langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "\n",
    "\n",
    "1. Create a .env file in the same folder as this notebook\n",
    "2. Add your OpenAI API key to the .env file in the format: `OPENAI_API_KEY=your_api_key_here`\n",
    "3. Run the code below to load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to load environment variables from the .env file\n",
    "# HINT: Use the dotenv library to load the .env file\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Working with Documents and Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Documents Manually\n",
    "\n",
    "In LangChain, a **Document** represents a unit of text and associated metadata. It has two main attributes:\n",
    "- **page_content**: a string representing the content\n",
    "- **metadata**: a dictionary containing arbitrary metadata\n",
    "\n",
    "Let's create some documents manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the Document class from langchain_core.documents\n",
    "# Then create a list of documents about movies with appropriate metadata\n",
    "# HINT: Each document should have page_content with movie descriptions and metadata with details like year, director, rating, etc.\n",
    "\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# documents = [\n",
    "#     Document(\n",
    "#         page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "#         metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "#     ),\n",
    "#     Document(\n",
    "#         page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "#         metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "#     ),\n",
    "#     Document(\n",
    "#         page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "#         metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "#     ),\n",
    "#     Document(\n",
    "#         page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "#         metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "#     ),\n",
    "#     Document(\n",
    "#         page_content=\"Toys come alive and have a blast doing so\",\n",
    "#         metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "#     ),\n",
    "#     Document(\n",
    "#         page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "#         metadata={\n",
    "#             \"year\": 1979,\n",
    "#             \"director\": \"Andrei Tarkovsky\",\n",
    "#             \"genre\": \"thriller\",\n",
    "#             \"rating\": 9.9,\n",
    "#         },\n",
    "#     ),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Vector Store (Chroma) and Persisting Documents\n",
    "\n",
    "**Chroma** is an AI-native open-source vector database. It can run in different modes:\n",
    "- **in-memory**: temporary storage that disappears when the session ends\n",
    "- **in-memory with persistence**: store/load to disk from a script or notebook\n",
    "- **in a docker container**: as a server running on your local machine or in the cloud\n",
    "\n",
    "Let's create a vector store using Chroma and store our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libraries and create a Chroma vector store\n",
    "# HINT: You need to import Chroma and either OpenAIEmbeddings or HuggingFaceEmbeddings\n",
    "# Then create a vector store from the documents and persist it to a directory\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Choose one of the embedding options below:\n",
    "# embeddings = OpenAIEmbeddings()  # If you have an OpenAI API key\n",
    "# embeddings = HuggingFaceEmbeddings()  # Free alternative using HuggingFace\n",
    "\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents, embedding=embeddings, persist_directory=\"chromadb_practice\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Vector Store from a Persist Directory\n",
    "\n",
    "Now let's see how to load a vector store from a persist directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the Chroma vector store from the persist directory you created above\n",
    "# HINT: Use the Chroma constructor with the persist_directory and embedding_function parameters\n",
    "\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=\"chromadb_practice\", embedding_function=embeddings\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Vector Store as a Retriever\n",
    "\n",
    "In LangChain:\n",
    "- VectorStore objects do not subclass Runnable\n",
    "- Retrievers are Runnables that can be used in chains with LCEL (LangChain Expression Language)\n",
    "\n",
    "Vector stores implement an **as_retriever** method that generates a Retriever (specifically a VectorStoreRetriever).\n",
    "\n",
    "Let's create a retriever from our vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a retriever from the vector store and test it with a query\n",
    "# HINT: Use the as_retriever method with appropriate search_type and search_kwargs parameters\n",
    "# Then invoke the retriever with a test query\n",
    "\n",
    "# retriever = vectorstore.as_retriever(\n",
    "#     search_type=\"similarity\",  # Other options: mmr, similarity_score_threshold\n",
    "#     search_kwargs={\"k\": 1},  # Return 1 most similar document\n",
    "# )\n",
    "\n",
    "# # Test the retriever with a query\n",
    "# retriever.invoke(\"tell me about movie directed by Satoshi Kon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading and Indexing PDF Documents with FAISS\n",
    "\n",
    "Now let's see how to load a PDF document, split it into chunks, and create a vector store using FAISS.\n",
    "\n",
    "### Loading the PDF\n",
    "\n",
    "First, make sure you have a PDF file named `travel-policy.pdf` in the same directory as this notebook or provide the correct path to your PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import PyPDFLoader and load documents from a PDF file\n",
    "# HINT: Use PyPDFLoader from langchain_community.document_loaders.pdf\n",
    "\n",
    "# from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# # Update the file path to your PDF file\n",
    "# loader = PyPDFLoader(file_path=\"travel-policy.pdf\")\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print one of the loaded documents to see its content\n",
    "# HINT: Try accessing documents[1] to see the second document\n",
    "\n",
    "# documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Documents into Chunks\n",
    "\n",
    "After loading documents, we need to split them into smaller chunks for better retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a text splitter and split the loaded documents into chunks\n",
    "# HINT: Use CharacterTextSplitter with appropriate chunk size and overlap\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50, separator=\"\\n\")\n",
    "# docs = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a FAISS Vector Store\n",
    "\n",
    "Now let's create a FAISS vector store from the document chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import FAISS and create a vector store from the document chunks\n",
    "# HINT: Use FAISS.from_documents and save it locally\n",
    "\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "# from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Choose one embedding option:\n",
    "# embeddings = OpenAIEmbeddings()  # If you have an OpenAI API key\n",
    "# embeddings = HuggingFaceEmbeddings()  # Free alternative using HuggingFace\n",
    "\n",
    "# vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "# vectorstore.save_local(\"faiss_store_practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Retrieval Chain using Vector Store\n",
    "\n",
    "Now let's create a retrieval chain using our vector store as a retriever. This is the core of a RAG (Retrieval Augmented Generation) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import necessary modules for creating a retrieval chain\n",
    "# HINT: You need to import ChatPromptTemplate, RunnablePassthrough, ChatOpenAI, and functions to create chains\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "# from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# # Load the saved FAISS vector store\n",
    "# vectorstore = FAISS.load_local(\n",
    "#     \"faiss_store_practice\", embeddings=embeddings, allow_dangerous_deserialization=True\n",
    "# )\n",
    "\n",
    "# # Create a retriever\n",
    "# retriever = vectorstore.as_retriever(\n",
    "#     search_type=\"similarity\",  # Other options: mmr, similarity_score_threshold\n",
    "#     search_kwargs={\"k\": 3},  # Return 3 most similar documents\n",
    "# )\n",
    "\n",
    "# # Create a prompt template for the RAG system\n",
    "# message = \"\"\"\n",
    "#         Answer this question using the provided context only.\n",
    "#         If the information is not available in the context, just reply with \"I don't know\"\n",
    "#         {input}\n",
    "#         Context:\n",
    "#         {context}\n",
    "#         \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "# # Create a language model\n",
    "# llm = ChatOpenAI()\n",
    "\n",
    "# # Create a question-answering chain that combines retrieved documents\n",
    "# question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# # Create the final RAG chain by combining the retriever and the question-answering chain\n",
    "# rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "# print(rag_chain)\n",
    "\n",
    "# # Alternative way to create a RAG chain using LCEL:\n",
    "# # rag_chain = (\n",
    "# #    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "# #    | prompt\n",
    "# #    | llm\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Retrieval Chain\n",
    "\n",
    "Now let's test our RAG chain with a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Invoke the RAG chain with a query and display the results\n",
    "# HINT: Use the invoke method with a dictionary containing the input key\n",
    "\n",
    "# response = rag_chain.invoke({\"input\": \"tell me about all the reimbursement policies\"})\n",
    "# print(response)\n",
    "# print(\"\\nAnswer:\")\n",
    "# print(response['answer'])\n",
    "# print(\"\\nRetrieved contexts:\")\n",
    "# for doc in response[\"context\"]:\n",
    "#     print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building a ChatPDF Application\n",
    "\n",
    "Now let's build an application similar to ChatPDF that allows users to upload a PDF and chat with it. We'll use Gradio to create a simple web interface.\n",
    "\n",
    "### Creating a PDF Loading Function\n",
    "\n",
    "First, let's create a function to load a PDF into a vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function to load a PDF into a vector store\n",
    "# HINT: The function should take a file (tempfile), load it, split it into chunks, and store it in a vector store\n",
    "\n",
    "# import tempfile\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# def load_pdf_into_vectorstore(file: tempfile) -> str:\n",
    "#     try:\n",
    "#         print(\"======Loading file==================\")\n",
    "#         file_path = file.name\n",
    "#         loader = PyPDFLoader(file_path=file_path)\n",
    "#         documents = loader.load()\n",
    "#         text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "#         docs = text_splitter.split_documents(documents=documents)\n",
    "#         embeddings = HuggingFaceEmbeddings()  # or OpenAIEmbeddings() if you have an API key\n",
    "\n",
    "#         # Choose one option below:\n",
    "#         # Option 1: FAISS\n",
    "#         # vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "#         # vectorstore.save_local(\"pdf_store\")\n",
    "\n",
    "#         # Option 2: Chroma\n",
    "#         vectorstore = Chroma.from_documents(\n",
    "#             documents, embedding=embeddings, persist_directory=\"chromadb_chatpdf\"\n",
    "#         )\n",
    "\n",
    "#         print(\"======File Loaded================== \")\n",
    "\n",
    "#         return 'Document uploaded and index created successfully. You can chat now.'\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         return str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Response Function\n",
    "\n",
    "Now let's create a function to handle chat queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function to get responses from the RAG system for chat queries\n",
    "# HINT: The function should load the vector store, create a RAG chain, and process the query\n",
    "\n",
    "# import gradio as gr\n",
    "# from langchain import OpenAI, PromptTemplate\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# model = ChatOpenAI()\n",
    "\n",
    "# def getresponse(query, history: list) -> tuple:\n",
    "#     # Load the vector store\n",
    "#     vectorstore = Chroma(\n",
    "#         persist_directory=\"chromadb_chatpdf\", embedding_function=embeddings\n",
    "#     )\n",
    "#     # Alternative: FAISS\n",
    "#     # vectorstore = FAISS.load_local(\"pdf_store\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "#     # Create the prompt template\n",
    "#     message = \"\"\"\n",
    "#     Answer this question using the provided context. If information is not available in the context,\n",
    "#     just respond saying \"I don't know\"\n",
    "#     {input}\n",
    "#     Context:\n",
    "#     {context}\n",
    "#     \"\"\"\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "#     llm = ChatOpenAI()\n",
    "\n",
    "#     # Create the RAG chain\n",
    "#     question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "#     rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)\n",
    "\n",
    "#     # Process the query\n",
    "#     response = rag_chain.invoke({\"input\": query})\n",
    "#     print(response)\n",
    "#     history.append((query, response['answer']))\n",
    "#     return \"\", history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Gradio Interface\n",
    "\n",
    "Now let's build a Gradio interface for our ChatPDF application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a Gradio interface for the ChatPDF application\n",
    "# HINT: Use gr.Blocks to create a UI with file upload, chat, and other components\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     with gr.Row():\n",
    "#         with gr.Column():\n",
    "#             file = gr.components.File(\n",
    "#                 label='Upload your PDF file',\n",
    "#                 file_count='single',\n",
    "#                 file_types=['.pdf'])\n",
    "#             upload = gr.components.Button(\n",
    "#                 value='Upload', variant='primary')\n",
    "#         label = gr.components.Textbox()\n",
    "#     \n",
    "#     chatbot = gr.Chatbot(label='Talk to the Document')\n",
    "#     msg = gr.Textbox()\n",
    "#     clear = gr.ClearButton([msg, chatbot])\n",
    "#     vectorStore = None\n",
    "\n",
    "#     # Connect components to functions\n",
    "#     upload.click(load_pdf_into_vectorstore, [file], [label])\n",
    "#     msg.submit(getresponse, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Retrieving Data from Web URLs\n",
    "\n",
    "Let's see how to retrieve data from a web URL, create embeddings, and use it for retrievals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a web loader to fetch and process content from a URL\n",
    "# HINT: Use WebBaseLoader and RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # Load blog post (or any other web page)\n",
    "# loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "# data = loader.load()\n",
    "\n",
    "# # Split the content into smaller chunks\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# splits = text_splitter.split_documents(data)\n",
    "\n",
    "# # Create a vector database\n",
    "# embedding = HuggingFaceEmbeddings()  # or OpenAIEmbeddings()\n",
    "# vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "# vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Using MultiQueryRetriever\n",
    "\n",
    "The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and use a MultiQueryRetriever\n",
    "# HINT: Import MultiQueryRetriever and create an instance from a vector store and LLM\n",
    "\n",
    "# from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# question = \"What are the approaches to Task Decomposition?\"\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "# retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "#     retriever=vectordb.as_retriever(), llm=llm\n",
    "# )\n",
    "\n",
    "# # Set up logging to see the multiple queries that are generated\n",
    "# import logging\n",
    "# logging.basicConfig()\n",
    "# logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# # Use the retriever\n",
    "# unique_docs = retriever_from_llm.invoke(question)\n",
    "# print(unique_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
