{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **In this lab, you will understand the basics of <font color=\"red\"> Langchain. </font>**\n",
        "\n",
        "This is a hands-on lab where you'll learn the fundamentals of LangChain, a framework for building applications with Large Language Models (LLMs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Install the required libraries for this lab\n",
        "# Run this cell to install langchain, langchain_openai, langsmith, and python-dotenv\n",
        "\n",
        "pip install langchain langchain_openai langsmith python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up your environment\n",
        "\n",
        "If you are using Jupyter notebook, follow the below instructions. Else skip this step and go to next step.\n",
        "\n",
        "**IMPORTANT: You need an OpenAI API key to complete this lab**\n",
        "\n",
        "1. Open the `.env` file in this folder\n",
        "2. Replace the placeholder with your own OpenAI API key or use the key provided by me\n",
        "3. The `.env` file should look like: `OPENAI_API_KEY=your-api-key-here`\n",
        "\n",
        "In the next cell, we'll load this key from the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load environment variables from .env file\n",
        "# HINT: Use the load_dotenv() function\n",
        "\n",
        "from dotenv import load_dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic LLM Interaction\n",
        "\n",
        "Now we'll use LangChain to interact with OpenAI's GPT model. We'll send a simple translation request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Send messages to GPT API using LangChain\n",
        "# HINT: Use ChatOpenAI and message types from langchain_core.messages\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Create a list of messages with:\n",
        "# 1. A SystemMessage asking to translate from English to Hindi\n",
        "# 2. A HumanMessage with the content \"Hello!\"\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Initialize the language model\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Invoke the model with your messages and store the result\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Display the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notice that the response from the model is an AIMessage.**\n",
        "\n",
        "This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. Let's see how to extract just the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Extract the string response from the AIMessage\n",
        "# HINT: Use StrOutputParser to get just the text content\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create a parser\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Get the result from the model\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Use the parser to extract just the text\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Chains\n",
        "\n",
        "LangChain allows us to create processing chains where the output of one component becomes the input to the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a chain that combines the LLM and the parser\n",
        "# HINT: Use the | operator to chain components together\n",
        "\n",
        "# Create a chain: llm | parser\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Invoke the chain with the same messages as before\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Prompt Templates\n",
        "\n",
        "Prompt templates allow us to create reusable prompts with placeholders for variables.\n",
        "\n",
        "Let's create a PromptTemplate that will take in two user variables:\n",
        "- <font color=\"red\">**language**</font>: The language to translate text into\n",
        "- <font color=\"red\">**text**</font>: The text to translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a ChatPromptTemplate for translation\n",
        "# HINT: Use ChatPromptTemplate.from_messages() with system and user messages\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a prompt template with placeholders for language and text\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Test the template by invoking it with Chinese as the language and \"hi\" as the text\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Display the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chaining Prompt Templates, LLMs, and Parsers\n",
        "\n",
        "Now let's combine all three components into a single chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a chain that combines prompt template, LLM, and parser\n",
        "# HINT: Use the | operator to chain all three components\n",
        "\n",
        "# Create a chain: prompt_template | llm | parser\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Invoke the chain with Chinese as the language and \"hi\" as the text\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Structured Output with Pydantic\n",
        "\n",
        "LangChain can use Pydantic models to structure the output from LLMs. This is useful when you need the LLM to return data in a specific format.\n",
        "\n",
        "For more information on FewShotPromptTemplate, see:\n",
        "https://python.langchain.com/docs/how_to/few_shot_examples/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define data models using Pydantic\n",
        "# HINT: Use BaseModel and Field from pydantic\n",
        "\n",
        "from typing import List\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Define a Person class with name and height_in_meters fields\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Define a People class that contains a list of Person objects\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Set up a parser for the People model\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Print the format instructions\n",
        "print(parser.get_format_instructions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a prompt that includes the format instructions\n",
        "# HINT: Use ChatPromptTemplate.from_messages() and .partial()\n",
        "\n",
        "from IPython.display import HTML, Markdown\n",
        "\n",
        "# Create a prompt that includes the format instructions\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Display the prompt with a sample query\n",
        "Markdown(prompt.invoke({\"query\": \"Rishik is 13 years old and she is 6 feet tall. Siva is 43 years old and 5.7 feet tall\"}).messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a chain using the prompt and LLM, then invoke it\n",
        "# HINT: Use the | operator to chain the prompt and LLM\n",
        "\n",
        "# Create a chain: prompt | llm\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Define a query about people and their heights\n",
        "\n",
        "# Your code here:\n",
        "\n",
        "\n",
        "# Invoke the chain with the query and display the result using Markdown\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caching LLM Responses\n",
        "\n",
        "LangChain can cache responses from LLMs to save time and API costs when making repeated requests.\n",
        "\n",
        "For more information about caching, see:\n",
        "https://python.langchain.com/docs/how_to/chat_model_caching/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Install the langchain-community package which contains caching utilities\n",
        "\n",
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Set up in-memory caching for LLM responses\n",
        "# HINT: Use set_llm_cache with InMemoryCache\n",
        "\n",
        "%%time\n",
        "from langchain.globals import set_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "# Set up the cache\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Make an LLM request and measure the time\n",
        "# HINT: Use the %%time magic command to measure execution time\n",
        "\n",
        "%%time\n",
        "# Make a request to the LLM\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Make the same request again and observe the time difference\n",
        "# HINT: The second request should be much faster due to caching\n",
        "\n",
        "%%time\n",
        "# Make the same request again\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this lab, you've learned:\n",
        "1. How to set up LangChain with OpenAI\n",
        "2. How to send basic messages to an LLM\n",
        "3. How to use output parsers to extract text\n",
        "4. How to create and use prompt templates\n",
        "5. How to chain components together\n",
        "6. How to use Pydantic for structured output\n",
        "7. How to implement caching for LLM responses\n",
        "\n",
        "These fundamentals will help you build more complex applications with LangChain!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
